{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmokeTest Granite TS -  TinyTimeMixer (TTM)\n",
    "\n",
    "This notebooke demonstrates the usage of a pre-trained `TinyTimeMixer` model for several multivariate time series forecasting tasks. For details related to model architecture, refer to the [TTM paper](https://arxiv.org/pdf/2401.03955.pdf).\n",
    "\n",
    "In this example, we will use a pre-trained TTM-512-96 model. That means the TTM model can take an input of 512 time points (`context_length`), and can forecast upto 96 time points (`forecast_length`) in the future. We will use the pre-trained TTM in two settings:\n",
    "1. **Zero-shot**: The pre-trained TTM will be directly used to evaluate on the `test` split of the target data. Note that the TTM was NOT pre-trained on the target data.\n",
    "2. **Few-shot**: The pre-trained TTM will be quickly fine-tuned on only 5% of the `train` split of the target data, and subsequently, evaluated on the `test` part of the target data.\n",
    "\n",
    "Note: Alternatively, this notebook can be modified to try the TTM-1024-96 model.\n",
    "\n",
    "Pre-trained TTM models will be fetched from the [Hugging Face TTM Model Repository](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the tsfm library\n",
    "! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.8\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    "    get_datasets,\n",
    ")\n",
    "from tsfm_public.toolkit.visualization import plot_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# DATA ROOT PATH\n",
    "# Make sure to download the target data (here ettm2) on the `DATA_ROOT_PATH` folder.\n",
    "# ETT is available at: https://github.com/zhouhaoyi/ETDataset/tree/main\n",
    "target_dataset = \"ettm2\"\n",
    "DATA_ROOT_PATH = \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv\"\n",
    "\n",
    "# Results dir\n",
    "OUT_DIR = \"ttm_finetuned_models/\"\n",
    "\n",
    "# TTM model branch\n",
    "# Use main for 512-96 model\n",
    "# Use \"1024_96_v1\" for 1024-96 model\n",
    "TTM_MODEL_REVISION = \"main\"\n",
    "\n",
    "# Forecasting parameters\n",
    "context_length = 512\n",
    "forecast_length = 96\n",
    "fewshot_fraction = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data file and see the columns\n",
    "df_tmp = pd.read_csv(DATA_ROOT_PATH)\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.iloc[:1000].plot(x=\"date\", y=\"HUFL\", figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = DATA_ROOT_PATH\n",
    "timestamp_column = \"date\"\n",
    "id_columns = []\n",
    "target_columns = [\"HUFL\", \"HULL\", \"MUFL\", \"MULL\", \"LUFL\", \"LULL\", \"OT\"]\n",
    "split_config = {\n",
    "    \"train\": [0, 12 * 30 * 24 * 4],\n",
    "    \"valid\": [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4],\n",
    "    \"test\": [\n",
    "        12 * 30 * 24 * 4 + 4 * 30 * 24 * 4,\n",
    "        12 * 30 * 24 * 4 + 8 * 30 * 24 * 4,\n",
    "    ],\n",
    "}\n",
    "# Understanding the split config -- slides\n",
    "\n",
    "data = pd.read_csv(\n",
    "    dataset_path,\n",
    "    parse_dates=[timestamp_column],\n",
    ")\n",
    "\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": timestamp_column,\n",
    "    \"id_columns\": id_columns,\n",
    "    \"target_columns\": target_columns,\n",
    "    \"control_columns\": [],\n",
    "}\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_length,\n",
    "    scaling=True,\n",
    "    encode_categorical=False,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = get_datasets(\n",
    "    tsp, data, split_config, fewshot_fraction=fewshot_fraction, fewshot_location=\"first\"\n",
    ")\n",
    "print(f\"Data lengths: train = {len(train_dataset)}, val = {len(valid_dataset)}, test = {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot evaluation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    \"ibm-granite/granite-timeseries-ttm-v1\", revision=TTM_MODEL_REVISION\n",
    ")\n",
    "zeroshot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "# zeroshot_trainer\n",
    "zeroshot_trainer = Trainer(\n",
    "    model=zeroshot_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=temp_dir,\n",
    "        per_device_eval_batch_size=64,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_predictions(\n",
    "    model=zeroshot_trainer.model,\n",
    "    dset=test_dataset,\n",
    "    plot_dir=os.path.join(OUT_DIR, \"ettm2\"),\n",
    "    plot_prefix=\"test_zeroshot\",\n",
    "    channel=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
